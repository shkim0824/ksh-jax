{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyNTFeY7LfXoW/9yhUdargh0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shkim0824/ksh-jax/blob/main/Flow_Matching_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "t71ZiBvdnsN8"
      },
      "outputs": [],
      "source": [
        "#@title Import package\n",
        "# jax/flax/optax import\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, random, lax\n",
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "\n",
        "# torch import\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Other Machine Learning Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Python extension libraires\n",
        "import math\n",
        "import time\n",
        "from functools import partial\n",
        "import pickle\n",
        "from typing import Any, Callable, Sequence, List\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Augmentations\n",
        "class TransformChain(object):\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\"\n",
        "        Apply transform sequentially\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, rng, image):\n",
        "        for _t in self.transforms:\n",
        "            image = _t(rng, image)\n",
        "        return image\n",
        "\n",
        "class ToTensorTransform(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Make image values between 0. ~ 1.\n",
        "        \"\"\"\n",
        "\n",
        "    def __call__(self, rng, image):\n",
        "        return image / 255.\n",
        "\n",
        "class RandomHFlipTransform(object):\n",
        "    def __init__(self, prob=0.5):\n",
        "        \"\"\"\n",
        "        Flip the image horizontally with the given probability\n",
        "\n",
        "        Inputs:\n",
        "            prob (float): probability of the flip\n",
        "        \"\"\"\n",
        "        self.prob=prob\n",
        "\n",
        "    def __call__(self, rng, image):\n",
        "        return jnp.where(\n",
        "            jax.random.bernoulli(rng, self.prob), # 1 for prob=self.prob\n",
        "            jnp.flip(image, axis=1), # Flip image\n",
        "            image, # Or not\n",
        "        )\n",
        "\n",
        "class RandomCropTransform(object):\n",
        "\n",
        "    def __init__(self, size, padding):\n",
        "        \"\"\"\n",
        "        Crop the image at a random location with given size and padding.\n",
        "        Inputs:\n",
        "            size (int): desired output size of the crop.\n",
        "            padding (int): padding on each border of the image before cropping.\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.padding = padding\n",
        "\n",
        "    def __call__(self, rng, image):\n",
        "        # Add padding\n",
        "        image = jnp.pad(\n",
        "            array           = image,\n",
        "            pad_width       = ((self.padding, self.padding),\n",
        "                               (self.padding, self.padding),\n",
        "                               (           0,            0),), # No pad for RGB channel\n",
        "            mode            = 'constant',\n",
        "            constant_values = 0,\n",
        "        )\n",
        "\n",
        "        # Random cropping position\n",
        "        rng1, rng2 = jax.random.split(rng, 2)\n",
        "        h0 = jax.random.randint(rng1, shape=(1,), minval=0, maxval=2*self.padding+1)[0] # output of randint is [x]. We get item of x\n",
        "        w0 = jax.random.randint(rng2, shape=(1,), minval=0, maxval=2*self.padding+1)[0]\n",
        "\n",
        "        # Slice image\n",
        "        image = jax.lax.dynamic_slice(\n",
        "            operand       = image,\n",
        "            start_indices = (h0, w0, 0), # We do not crop rgb channel\n",
        "            slice_sizes   = (self.size, self.size, image.shape[2]), # We do not crop rgb channel\n",
        "        )\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "id": "Kgnfb5eRw0C3",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy loss\n",
        "def evaluate_ce(softmax, one_hot):\n",
        "    return jnp.mean(-jnp.sum(one_hot * jnp.log(softmax+1e-12), axis=-1))\n",
        "\n",
        "# Test Accuracy\n",
        "def evaluate_acc(logits, labels):\n",
        "    return jnp.mean(jnp.argmax(logits, axis=1) == labels)"
      ],
      "metadata": {
        "id": "9TNuHsdFdYJu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Data\n",
        "def buildLoader(images, labels, batch_size, steps_per_epoch, rng=None, shuffle=False, transform=None):\n",
        "    # Shuffle Indices\n",
        "    indices = jax.random.permutation(rng, len(images)) if shuffle else jnp.arange(len(images)) # Make shuffled indices\n",
        "    indices = indices[:steps_per_epoch*batch_size] # Batch size may not be divisor of length of images. We drop left ones.\n",
        "    indices = indices.reshape((steps_per_epoch, batch_size,))\n",
        "    for batch_idx in indices:\n",
        "        batch = {'images': jnp.array(images[batch_idx]), 'labels': jnp.array(labels[batch_idx])}\n",
        "        if transform is not None:\n",
        "            if rng is not None:\n",
        "                _, rng = jax.random.split(rng)\n",
        "            sub_rng = None if rng is None else jax.random.split(rng, batch['images'].shape[0])\n",
        "            batch['images'] = transform(sub_rng, batch['images'])\n",
        "        yield batch\n",
        "\n",
        "# Hyper parameters\n",
        "BATCH_SIZE = 128\n",
        "TEST_SIZE = 10000\n",
        "VAL_SIZE = 128\n",
        "n_targets = 100\n",
        "rng = random.PRNGKey(0)\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
        "test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
        "\n",
        "# Make validation set\n",
        "train_images, val_images = train_test_split(train_set.data, test_size=1000, random_state=42)\n",
        "train_labels, val_labels = train_test_split(train_set.targets, test_size=1000, random_state=42) # It very boost training speed\n",
        "\n",
        "# We use img converted to jnp dtype\n",
        "train_images = np.array(train_images, dtype=jnp.float32)\n",
        "val_images = np.array(val_images, dtype=jnp.float32)\n",
        "test_images = np.array(test_set.data, dtype=jnp.float32)\n",
        "\n",
        "# Make labels to numpy array\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "test_labels = np.array(test_set.targets)\n",
        "\n",
        "# transform for CIFAR-10\n",
        "transform = TransformChain([RandomHFlipTransform(0.5),\n",
        "                            RandomCropTransform(size=32, padding=4),\n",
        "                            ToTensorTransform()])\n",
        "\n",
        "# Naive data loader. We should put rng later in real-time usage\n",
        "trn_steps_per_epoch = len(train_images) // BATCH_SIZE\n",
        "tst_steps_per_epoch = len(test_images) // TEST_SIZE\n",
        "val_steps_per_epoch = len(val_images) // VAL_SIZE\n",
        "\n",
        "trn_loader = partial(buildLoader,\n",
        "                    images=train_images,\n",
        "                    labels=train_labels,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    steps_per_epoch=trn_steps_per_epoch,\n",
        "                    shuffle=True,\n",
        "                    transform=jit(vmap(ToTensorTransform())))\n",
        "\n",
        "trn_loader_aug = partial(buildLoader,\n",
        "                        images=train_images,\n",
        "                        labels=train_labels,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        steps_per_epoch=trn_steps_per_epoch,\n",
        "                         shuffle=True,\n",
        "                         transform=jit(vmap(transform)))\n",
        "\n",
        "vl_loader = partial(buildLoader,\n",
        "                    images=val_images,\n",
        "                    labels=val_labels,\n",
        "                    batch_size=VAL_SIZE,\n",
        "                    steps_per_epoch=val_steps_per_epoch,\n",
        "                    shuffle=True,\n",
        "                    transform=jit(vmap(ToTensorTransform())))\n",
        "\n",
        "tst_loader = partial(buildLoader,\n",
        "                    images=test_images,\n",
        "                    labels=test_labels,\n",
        "                    batch_size=TEST_SIZE,\n",
        "                    steps_per_epoch=tst_steps_per_epoch,\n",
        "                    shuffle=False,\n",
        "                    transform=jit(vmap(ToTensorTransform())))"
      ],
      "metadata": {
        "id": "dqq7FOHNw6aG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Denoising MLP\n",
        "from einops import rearrange\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = jnp.exp(\n",
        "        -math.log(max_period) * jnp.arange(0, half, dtype=jnp.float32) / half\n",
        "    )\n",
        "    args = timesteps[..., None] * freqs[None]\n",
        "    embedding = jnp.concatenate([jnp.cos(args), jnp.sin(args)], axis=-1)\n",
        "    if len(timesteps.shape) == 2:\n",
        "        embedding = rearrange(embedding, \"b n d -> b (n d)\")\n",
        "    if dim % 2:\n",
        "        embedding = jnp.concatenate(\n",
        "            [embedding, jnp.zeros_like(embedding[:, :1])], axis=-1)\n",
        "    return embedding\n",
        "\n",
        "class DenoisingMLP(nn.Module):\n",
        "    \"\"\"3개의 ResidualBlock과 폭(채널 수) 1024를 가진 단순 MLP 예시.\"\"\"\n",
        "    hidden_dim: int = 1024\n",
        "    num_blocks: int = 3\n",
        "    num_classes: int = 100\n",
        "    dtype: Any = jnp.float32\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, t, c):\n",
        "        \"\"\"\n",
        "        x: 입력 노이즈(혹은 중간 상태) [batch, features]\n",
        "        t: time step (예: 스칼라 혹은 [batch])\n",
        "        z: AR/MAR에서 나오는 condition vector [batch, cond_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = jnp.concatenate([x, c], axis=-1)\n",
        "\n",
        "        for i in range(self.num_blocks):\n",
        "            x_skip = x\n",
        "\n",
        "            t = jnp.log((1 - t) + 1e-12) / 4\n",
        "\n",
        "            # (1) Adaptive Layer Normalization utilizing time-embedding\n",
        "            t_emb = timestep_embedding(t, dim=64)\n",
        "            t_ = nn.Dense(2 * x.shape[-1], kernel_init=nn.initializers.constant(0.))(t)\n",
        "            t_ = nn.silu(t_)\n",
        "            shift_t, scale_t = jnp.split(t_, 2, axis=-1)\n",
        "\n",
        "            x = nn.LayerNorm(use_bias=False, use_scale=False)(x)\n",
        "            x = x * (1 + scale_t) + shift_t\n",
        "\n",
        "            # (2) Residual Block\n",
        "            x = nn.Dense(\n",
        "                    features=self.hidden_dim,\n",
        "                    dtype=self.dtype,\n",
        "                    kernel_init=nn.initializers.xavier_uniform(),\n",
        "                    bias_init=nn.initializers.normal(stddev=1e-6)\n",
        "                )(x)\n",
        "            x = nn.silu(x)\n",
        "            x = nn.Dense(\n",
        "                    features=self.hidden_dim,\n",
        "                    dtype=self.dtype,\n",
        "                    kernel_init=nn.initializers.xavier_uniform(),\n",
        "                    bias_init=nn.initializers.normal(stddev=1e-6)\n",
        "                )(x)\n",
        "            x = x_skip + x if i > 0 else x\n",
        "\n",
        "            # x = nn.Dropout(rate=self.droprate)(x, deterministic=not kwargs[\"training\"])\n",
        "\n",
        "        # (3) Final Logit\n",
        "        x = nn.Dense(\n",
        "                features=self.num_classes,\n",
        "                dtype=self.dtype,\n",
        "                kernel_init=nn.initializers.xavier_uniform(),\n",
        "                bias_init=nn.initializers.normal(stddev=1e-6)\n",
        "            )(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "m19NIfSYw-R1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ResNet\n",
        "\n",
        "class IdentityShortcut(nn.Module):\n",
        "    channels: int\n",
        "    strides: int\n",
        "    expansion: int = 1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        pad_offset = self.expansion * self.channels - x.shape[-1] # x.shape[-1] is original channel dimension\n",
        "        return jnp.pad(\n",
        "            array           = x[:, ::self.strides, ::self.strides, :], # get reduced shape of x\n",
        "            pad_width       = ((0,0), (0,0), (0,0), (0, pad_offset)), # Add zero padding to channel\n",
        "            mode            = 'constant',\n",
        "            constant_values  = 0, # zero padding\n",
        "        )\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    channels: int\n",
        "    strides: int = 1\n",
        "    shortcut: nn.Module = IdentityShortcut # identity or projection\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training=False):\n",
        "        y = nn.Conv(features = self.channels, kernel_size=(3,3), strides=self.strides)(x)\n",
        "        y = nn.BatchNorm(use_running_average=not training)(y)\n",
        "        y = nn.relu(y)\n",
        "        y = nn.Conv(features = self.channels, kernel_size=(3,3))(y)\n",
        "        y = nn.BatchNorm(use_running_average=not training)(y)\n",
        "\n",
        "        if self.strides != 1 or x.shape[-1] != self.channels * 1: # We have to modify x to match shape\n",
        "            y = y + self.shortcut(channels  = self.channels,\n",
        "                                  strides    = self.strides,\n",
        "                                  expansion = 1)(x)\n",
        "        else:\n",
        "            y = y + x\n",
        "\n",
        "        y = nn.relu(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "class ResNet32(nn.Module):\n",
        "    \"\"\" ResNet-32 Structure for CIFAR-10\n",
        "    Starting Block: 3 by 3 conv, 16\n",
        "    Residual Block: 16 channel * n - 32 channel * n - 64 channel * n, n=5 for ResNet-32\n",
        "    \"\"\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training=False):\n",
        "        # Starting Block: 3 by 3 conv\n",
        "        x = nn.Conv(features = 16, kernel_size=(3,3))(x)\n",
        "        x = nn.BatchNorm(use_running_average=not training)(x)\n",
        "        x = nn.relu(x)\n",
        "\n",
        "        # Intermediate Residual Blocks\n",
        "        for i in range(5):\n",
        "            x = ResidualBlock(channels=16)(x, training)\n",
        "            # jax.debug.print(\"{}\", x.shape)\n",
        "\n",
        "        for channel in [32, 64]:\n",
        "            x = ResidualBlock(channels=channel, strides=2)(x, training)\n",
        "            for i in range(4):\n",
        "                x = ResidualBlock(channels=channel)(x, training)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        x = jnp.mean(x, [1, 2])\n",
        "        representation = x\n",
        "\n",
        "        # Classifier\n",
        "        x = nn.Dense(features=100)(x) # Classifier, return logits\n",
        "\n",
        "        return x, representation"
      ],
      "metadata": {
        "cellView": "form",
        "id": "U1OGw8YP5n5X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowTrainState(train_state.TrainState):\n",
        "    pass\n",
        "\n",
        "def create_dMLP_state(key, feature_dim=1024, cond_dim=64, lr=1e-4):\n",
        "    model = DenoisingMLP(\n",
        "        hidden_dim=feature_dim,\n",
        "        num_blocks=3,\n",
        "    )\n",
        "\n",
        "    dummy_x = jnp.zeros((1, 100), dtype=jnp.float32)\n",
        "    dummy_t = jnp.zeros((1, 1), dtype=jnp.float32)\n",
        "    dummy_z = jnp.zeros((1, cond_dim), dtype=jnp.float32)\n",
        "\n",
        "    variables = model.init(key, dummy_x, dummy_t, dummy_z)\n",
        "    params = variables[\"params\"]\n",
        "\n",
        "    # (3) 옵티마 + 스케줄 설정\n",
        "    warmup_epochs = 5\n",
        "    epochs = 50\n",
        "    trn_steps_per_epoch = len(train_images) // BATCH_SIZE  # 사용자 정의\n",
        "    scheduler = optax.join_schedules(\n",
        "        schedules = [\n",
        "            # warmup\n",
        "            optax.linear_schedule(\n",
        "                init_value       = 0.01 * lr,\n",
        "                end_value        = lr,\n",
        "                transition_steps = warmup_epochs * trn_steps_per_epoch\n",
        "            ),\n",
        "            # cosine decay\n",
        "            optax.cosine_decay_schedule(\n",
        "                init_value       = lr,\n",
        "                decay_steps      = (epochs - warmup_epochs) * trn_steps_per_epoch\n",
        "            )\n",
        "        ],\n",
        "        boundaries = [warmup_epochs * trn_steps_per_epoch]\n",
        "    )\n",
        "    tx = optax.adam(scheduler)\n",
        "\n",
        "    # (4) TrainState 생성\n",
        "    state = FlowTrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        params   = params,\n",
        "        tx       = tx\n",
        "    )\n",
        "    return state\n",
        "\n",
        "def create_res32(rng, params, batch_stats, lr_fn=0.1):\n",
        "    model = ResNet32()\n",
        "\n",
        "    class TrainState(train_state.TrainState): # We don't use dropout in ResNet\n",
        "        batch_stats: Any\n",
        "\n",
        "    state = TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        params = params,\n",
        "        batch_stats = batch_stats, # batch state\n",
        "        tx = optax.sgd(learning_rate=lr_fn, momentum=0.9, nesterov=True) # learning_rate_fn automatically determine learning rate\n",
        "    )\n",
        "    return state"
      ],
      "metadata": {
        "id": "zLHsauSUxNQL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title loss, train_step\n",
        "def flow_matching_loss(params, apply_fn, rng, teacher_logits, teacher_representations):\n",
        "    \"\"\"\n",
        "    teacher_logits: (B, K)\n",
        "    \"\"\"\n",
        "    B, K = teacher_logits.shape\n",
        "\n",
        "    # 노이즈 x0 ~ N(0,1)\n",
        "    rng, rng_x0, rng_t = jax.random.split(rng, 3)\n",
        "    x0 = jax.random.normal(rng_x0, shape=(B, K))\n",
        "\n",
        "    # t ~ Uniform(0,1)\n",
        "    t = jax.random.uniform(rng_t, shape=(B, 1))\n",
        "\n",
        "    # OT 경로: y_t = (1 - t)*x0 + t*x1\n",
        "    y_t = (1.0 - t)*x0 + t*teacher_logits\n",
        "\n",
        "    # 목표 벡터: x1 - x0\n",
        "    target_vec = teacher_logits - x0\n",
        "\n",
        "    # 모델 출력 v_theta(t, y_t)\n",
        "    pred_vec = apply_fn({'params': params}, y_t, t, teacher_representations)\n",
        "\n",
        "    # MSE\n",
        "    loss = jnp.mean((pred_vec - target_vec)**2)\n",
        "    return loss, (loss, rng)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(teacher, state, rng, images):\n",
        "    # (1) Teacher Logit, Representation\n",
        "    logit_t, rep_t = teacher.apply_fn(\n",
        "        {'params': teacher.params, 'batch_stats': teacher.batch_stats}, batch['images'], training=False)\n",
        "\n",
        "    # (2) Flow Matching Loss\n",
        "    def loss_fn(p):\n",
        "        return flow_matching_loss(p, state.apply_fn, rng, logit_t, rep_t)\n",
        "    (loss_val, aux), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "    new_rng = aux[1]\n",
        "    metrics = {'loss': loss_val}\n",
        "    return new_state, new_rng, metrics\n",
        "\n",
        "@jax.jit\n",
        "def test_step(teacher, state, rng, batch, steps=100):\n",
        "    # (1) Teacher Logit, Representation\n",
        "    logit_t, rep_t = teacher.apply_fn(\n",
        "        {'params': teacher.params, 'batch_stats': teacher.batch_stats}, batch['images'], training=False)\n",
        "\n",
        "    # (2) Flow matching Sampling\n",
        "    B = batch['images'].shape[0]\n",
        "\n",
        "    rng, key = jax.random.split(rng)\n",
        "    x = jax.random.normal(rng, shape=(B, 100))\n",
        "\n",
        "    ts = jnp.linspace(0.0, 1.0, steps+1)\n",
        "\n",
        "    for i in range(steps):\n",
        "        t0, t1 = ts[i], ts[i+1]\n",
        "        v = state.apply_fn({'params': state.params}, x, jnp.full((B, 1), t0), rep_t)\n",
        "        dt = (t1 - t0)\n",
        "        x = x + v * dt      # Euler Method\n",
        "\n",
        "    logits = x\n",
        "\n",
        "    one_hot = jax.nn.one_hot(batch['labels'], n_targets)\n",
        "    softmax = jax.nn.softmax(logits)\n",
        "\n",
        "    metrics = {\n",
        "        'acc': evaluate_acc(logits, batch['labels']),\n",
        "        'loss': evaluate_ce(softmax, one_hot)\n",
        "    }\n",
        "    return rng, metrics\n"
      ],
      "metadata": {
        "id": "YO55i7GYx9P0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teachers = []\n",
        "\n",
        "# 파라미터와 배치 정규화 상태 불러오기\n",
        "for i in range(5):\n",
        "    with open(f\"ResNet32 CIFAR100 params_{i}.pickle\", \"rb\") as fr:\n",
        "        params = pickle.load(fr)\n",
        "    with open(f\"ResNet32 CIFAR100 batch_stats_{i}.pickle\", \"rb\") as fr:\n",
        "        batch_stats = pickle.load(fr)\n",
        "    teachers.append(create_res32(jax.random.PRNGKey(i), params, batch_stats))"
      ],
      "metadata": {
        "id": "ECpS7B8CE8j2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = create_dMLP_state(rng)"
      ],
      "metadata": {
        "id": "CJjYO04D4K1-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_history = defaultdict(list)\n",
        "for epoch in range(50):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Make data laoder\n",
        "    rng, *keys = random.split(rng, 4)\n",
        "    train_loader = trn_loader(rng=keys[0])\n",
        "    test_loader = tst_loader(rng=keys[1])\n",
        "    val_loader = vl_loader(rng=keys[2])\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # teacher selection\n",
        "        rng, key = jax.random.split(rng)\n",
        "        t_idx = jax.random.randint(key, shape=(), minval=0, maxval=len(teachers))\n",
        "        teacher = teachers[t_idx]\n",
        "\n",
        "        # train\n",
        "        state, rng, metrics = train_step(teacher, state, rng, batch['images']) # Use key for training\n",
        "    for metric, value in metrics.items(): # compute metrics\n",
        "        metrics_history[f'train_{metric}'].append(value) # record metrics\n",
        "\n",
        "    tst_metrics = defaultdict(float)\n",
        "    for batch in test_loader:\n",
        "        # teacher selection\n",
        "        rng, key = jax.random.split(rng)\n",
        "        t_idx = jax.random.randint(key, shape=(), minval=0, maxval=len(teachers))\n",
        "        teacher = teachers[t_idx]\n",
        "\n",
        "        rng, metrics = test_step(teacher, state, rng, batch) # In test, we do not need key\n",
        "        for metric, value in metrics.items(): # compute metrics\n",
        "            tst_metrics[f'test_{metric}'] += value\n",
        "    for metric, value in tst_metrics.items():\n",
        "        metrics_history[metric].append(tst_metrics[metric] / tst_steps_per_epoch)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1} in {epoch_time:.2f} sec\")\n",
        "    print(f\"Train loss: {metrics_history['train_loss'][-1]:.3f}\\n Test acc: {metrics_history['test_acc'][-1]:.3f}, Test loss: {metrics_history['test_loss'][-1]:.3f}, \")"
      ],
      "metadata": {
        "id": "T9M1NdHLzePc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6965e462-1b09-40ef-c155-d3515e66b35d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 in 72.23 sec\n",
            "Train loss: 5.508\n",
            " Test acc: 0.056, Test loss: 6.254, \n",
            "Epoch 2 in 1.20 sec\n",
            "Train loss: 4.140\n",
            " Test acc: 0.123, Test loss: 5.514, \n",
            "Epoch 3 in 33.38 sec\n",
            "Train loss: 2.767\n",
            " Test acc: 0.271, Test loss: 4.021, \n",
            "Epoch 4 in 33.54 sec\n",
            "Train loss: 1.791\n",
            " Test acc: 0.281, Test loss: 4.142, \n",
            "Epoch 5 in 33.36 sec\n",
            "Train loss: 1.458\n",
            " Test acc: 0.370, Test loss: 3.125, \n",
            "Epoch 6 in 1.27 sec\n",
            "Train loss: 1.009\n",
            " Test acc: 0.439, Test loss: 2.613, \n",
            "Epoch 7 in 34.26 sec\n",
            "Train loss: 0.887\n",
            " Test acc: 0.396, Test loss: 2.799, \n",
            "Epoch 8 in 1.28 sec\n",
            "Train loss: 0.706\n",
            " Test acc: 0.399, Test loss: 2.708, \n",
            "Epoch 9 in 1.30 sec\n",
            "Train loss: 0.669\n",
            " Test acc: 0.376, Test loss: 3.022, \n",
            "Epoch 10 in 1.14 sec\n",
            "Train loss: 0.640\n",
            " Test acc: 0.463, Test loss: 2.339, \n",
            "Epoch 11 in 1.17 sec\n",
            "Train loss: 0.598\n",
            " Test acc: 0.467, Test loss: 2.297, \n",
            "Epoch 12 in 1.20 sec\n",
            "Train loss: 0.618\n",
            " Test acc: 0.383, Test loss: 2.933, \n",
            "Epoch 13 in 1.26 sec\n",
            "Train loss: 0.594\n",
            " Test acc: 0.381, Test loss: 2.887, \n",
            "Epoch 14 in 1.34 sec\n",
            "Train loss: 0.635\n",
            " Test acc: 0.466, Test loss: 2.259, \n",
            "Epoch 15 in 1.31 sec\n",
            "Train loss: 0.457\n",
            " Test acc: 0.419, Test loss: 2.463, \n",
            "Epoch 16 in 1.23 sec\n",
            "Train loss: 0.510\n",
            " Test acc: 0.450, Test loss: 2.332, \n",
            "Epoch 17 in 1.27 sec\n",
            "Train loss: 0.553\n",
            " Test acc: 0.468, Test loss: 2.221, \n",
            "Epoch 18 in 1.16 sec\n",
            "Train loss: 0.472\n",
            " Test acc: 0.420, Test loss: 2.428, \n",
            "Epoch 19 in 1.19 sec\n",
            "Train loss: 0.426\n",
            " Test acc: 0.396, Test loss: 2.752, \n",
            "Epoch 20 in 1.23 sec\n",
            "Train loss: 0.478\n",
            " Test acc: 0.398, Test loss: 2.749, \n",
            "Epoch 21 in 1.20 sec\n",
            "Train loss: 0.447\n",
            " Test acc: 0.467, Test loss: 2.199, \n",
            "Epoch 22 in 1.13 sec\n",
            "Train loss: 0.460\n",
            " Test acc: 0.401, Test loss: 2.533, \n",
            "Epoch 23 in 1.20 sec\n",
            "Train loss: 0.398\n",
            " Test acc: 0.394, Test loss: 2.737, \n",
            "Epoch 24 in 1.20 sec\n",
            "Train loss: 0.468\n",
            " Test acc: 0.420, Test loss: 2.384, \n",
            "Epoch 25 in 1.35 sec\n",
            "Train loss: 0.442\n",
            " Test acc: 0.476, Test loss: 2.165, \n",
            "Epoch 26 in 1.28 sec\n",
            "Train loss: 0.412\n",
            " Test acc: 0.422, Test loss: 2.396, \n",
            "Epoch 27 in 1.29 sec\n",
            "Train loss: 0.398\n",
            " Test acc: 0.397, Test loss: 2.726, \n",
            "Epoch 28 in 1.12 sec\n",
            "Train loss: 0.385\n",
            " Test acc: 0.427, Test loss: 2.363, \n",
            "Epoch 29 in 1.08 sec\n",
            "Train loss: 0.334\n",
            " Test acc: 0.402, Test loss: 2.487, \n",
            "Epoch 30 in 1.03 sec\n",
            "Train loss: 0.374\n",
            " Test acc: 0.404, Test loss: 2.476, \n",
            "Epoch 31 in 1.05 sec\n",
            "Train loss: 0.336\n",
            " Test acc: 0.461, Test loss: 2.227, \n",
            "Epoch 32 in 1.16 sec\n",
            "Train loss: 0.343\n",
            " Test acc: 0.478, Test loss: 2.138, \n",
            "Epoch 33 in 1.21 sec\n",
            "Train loss: 0.348\n",
            " Test acc: 0.425, Test loss: 2.355, \n",
            "Epoch 34 in 1.37 sec\n",
            "Train loss: 0.307\n",
            " Test acc: 0.472, Test loss: 2.141, \n",
            "Epoch 35 in 1.30 sec\n",
            "Train loss: 0.287\n",
            " Test acc: 0.401, Test loss: 2.677, \n",
            "Epoch 36 in 1.23 sec\n",
            "Train loss: 0.363\n",
            " Test acc: 0.472, Test loss: 2.142, \n",
            "Epoch 37 in 1.18 sec\n",
            "Train loss: 0.277\n",
            " Test acc: 0.402, Test loss: 2.668, \n",
            "Epoch 38 in 1.31 sec\n",
            "Train loss: 0.303\n",
            " Test acc: 0.424, Test loss: 2.361, \n",
            "Epoch 39 in 1.29 sec\n",
            "Train loss: 0.285\n",
            " Test acc: 0.423, Test loss: 2.353, \n",
            "Epoch 40 in 1.21 sec\n",
            "Train loss: 0.309\n",
            " Test acc: 0.405, Test loss: 2.456, \n",
            "Epoch 41 in 1.18 sec\n",
            "Train loss: 0.291\n",
            " Test acc: 0.456, Test loss: 2.218, \n",
            "Epoch 42 in 1.25 sec\n",
            "Train loss: 0.273\n",
            " Test acc: 0.459, Test loss: 2.219, \n",
            "Epoch 43 in 1.05 sec\n",
            "Train loss: 0.308\n",
            " Test acc: 0.401, Test loss: 2.656, \n",
            "Epoch 44 in 1.18 sec\n",
            "Train loss: 0.306\n",
            " Test acc: 0.424, Test loss: 2.354, \n",
            "Epoch 45 in 1.21 sec\n",
            "Train loss: 0.274\n",
            " Test acc: 0.410, Test loss: 2.454, \n",
            "Epoch 46 in 1.35 sec\n",
            "Train loss: 0.302\n",
            " Test acc: 0.473, Test loss: 2.136, \n",
            "Epoch 47 in 1.37 sec\n",
            "Train loss: 0.291\n",
            " Test acc: 0.422, Test loss: 2.348, \n",
            "Epoch 48 in 1.20 sec\n",
            "Train loss: 0.312\n",
            " Test acc: 0.458, Test loss: 2.216, \n",
            "Epoch 49 in 1.26 sec\n",
            "Train loss: 0.258\n",
            " Test acc: 0.406, Test loss: 2.454, \n",
            "Epoch 50 in 1.24 sec\n",
            "Train loss: 0.286\n",
            " Test acc: 0.405, Test loss: 2.451, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTFbWmI9HnGT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}